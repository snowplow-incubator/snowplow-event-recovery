#+TITLE: Snowplow Event Recovery
#+STARTUP: hideblocks

This repository contains the codebases used for recovering bad rows emitted by a Snowplow pipeline.

The different Snowplow pipelines being all non-lossy, if something goes wrong during, for example,
schema validation or enrichment the payloads (alongside the errors that happened) are stored into a
bad rows storage solution, be it a data stream or object storage, instead of being discarded.

The goal of recovery is to fix the payloads contained in these bad rows so that they are ready to be
processed successfully by a Snowplow enrichment platform.

* Configurations
There is a wide range of use cases at which the recovery might come in handy. Configuration mechanism allows for flexibility taking into account the most common usecases.
For specific use cases see [[Extending recovery]] section.
** Steps
Steps are individual modifications applied to Bad Row payloads as atomic parts of recovery flows (scenarios).
The steps operate on specific field values and can replace or nullify/remove values.
Other modifications could possibly be implemented in the future.
There is prior art started at [[http://jsonpatch.com/#operations][Json Patch]] that provides an RFC for patching operations used with HTTP.
These operations are however limited to structural changes not value changes and it assumes no deep modifications.
It was suggested whether we could extend the specification to also operate internally on field values.
The assumption is that we add additional, optional field - `match` that, when available as a part of step description, will use the field's value as a regex and replace its matches with `value`.

Currently we do anticipate following operations that can be expressed:
- replace value contents by matching an expression (can be used to replace complete values)
- remove value contents by matching an expression
- cast value from one type to another (ie. string -> int, int -> string, a -> [a])

It is important to note that recovery operates on bad row payloads and thus [[Steps]]' path scopes its entry point as bad row's =payload= field.
#+begin_src javascript
{
  "op": "Replace",               // enumerated operations
  "path": "body",                // Json Path with an entry point in `payload` (!)
  "match": "(?U)^.*$",           // value to match
  "value": "3-body"              // new value
 },
 {
   "op": "Remove",               // enumerated operations
   "path": "body",               // Json Path with an entry point in `payload` (!)
   "match": "-body"              // value to match
 },
 {
   "op": "Cast",                 // enumerated operations
   "path": "body",               // Json Path with an entry point in `payload` (!)
   "from": "string",             // current type of $path
   "to": "int"                   // resulting type of $path
 }
#+end_src
** Conditions
Conditions are boolean expressions that operate on BadRow fields.
The conditions are used to select proper recovery flow depending on BadRow structure or value.
For structure finding appropriate path is solved by many specs of JSON query ie. Json Path.
The values can be matched using regexes using a flexible syntax that is well understood and widely used.
Condition therefore is a product of path and value expressions. To keep it in line with [[http://jsonpatch.com/#operations][JSON Patch]], following description is proposed:
It is important to note that conditions can be applied to arbitrary bad row fields and thus [[Conditions]]' path scopes its entry point as bad row's "root".

#+begin_src javascript
 {
     "op": "Test",
     "path": "payload.body", 
     "value": { "regex": ".*" }
 },
 {
     "op": "Test",
     "path": "payload.body", 
     "value": { "regex": ".*" }
 },
 {
     "op": "Test",
     "path": "payload.body", 
     "value": { "data": { "error": "RuntimeException" }  }
 },
 {
     "op": "Test",
     "path": "payload.body", 
     "value": { "size": { "eq": 3 } }
 },
 {
     "op": "Test",
     "path": "payload.body", 
     "value": { "size": { "gt": 3 } }
 }
#+end_src

Currently we do anticipate following operations that can be expressed:
- match a regular expression
- compare data directly (object equality)
- check size for equality, less or greater than
** Flows
Flows are sequences of Steps applied one by one.
** Full example
#+begin_src javascript
{
  "schema": "iglu:com.snowplowanalytics.snowplow/recoveries/jsonschema/2-0-0",
  "data": {
    "iglu:com.snowplowanalytics.snowplow.badrows/tracker_protocol_violation/jsonschema/1-*-*": [
        {
          "conditions": [
              {
                  "op": "test",
                  "path": "root.payload.body", 
                  "value": { "regex": ".*" }
              },
              {
                  "op": "test",
                  "path": "root.payload.body", 
                  "value": { "regex": ".*" }
              },
              {
                  "op": "test",
                  "path": "root.payload.body", 
                  "value": { "compare": { "error": "RuntimeException" }  }
              },
              {
                  "op": "test",
                  "path": "root.payload.body", 
                  "value": { "size": { "eq": 3 } }
              },
              {
                  "op": "test",
                  "path": "root.payload.body", 
                  "value": { "size": { "gt": 3 } }
              },
              {
                  "op": "test",
                  "path": "root.payload.body", 
                  "value": { "apply": "(x) => x.toUpperCase()" }
              }
           ],
           "steps": [
              {
                "op": "replace",
                "path": "root.payload.body",
                "match": "(?U)^.*$",
                "value": "3-body"
               },
               {
                 "op": "remove",
                 "path": "root.payload.body",
                 "match": "-body"
               },
               {
                 "op": "cast",
                 "path": "root.payload.body",
                 "from": "string",
                 "to": "int"
               },
               {
                 "op": "apply",
                 "path": "root.payload.body",
                 "value": "(x) => x.toUpperCase()"
               }
           ]
        }
     ]
  }
}
#+end_src
* Run
** Define config
For a job:
#+begin_src javascript :tangle bin/0-job-config.json
{
  "schema": "iglu:com.snowplowanalytics.snowplow/recoveries/jsonschema/2-0-0",
	"data": {
    "iglu:com.snowplowanalytics.snowplow.badrows/enrichment_failures/jsonschema/1-0-0": [
      {
        "name": "lorem-ipsum",
			  "conditions": [
          {
            "op": "Test",
            "path": "raw.vendor",
            "value": { 
              "value": "com.snowplowanalytics.snowplow"
            }
          }
        ],
        "steps": [
          {
            "op": "Replace",
            "path": "raw.vendor",
            "match": "(?U)^.*$",
            "value": "com.snowplowanalytics.snplow"
          }
        ]
		  },
      {
        "name": "dolor-sit-amet",
			  "conditions": [],
        "steps": [
          {
            "op": "Replace",
            "path": "raw.vendor",
            "match": "(?U)^.*$",
            "value": "com.snplow"
          }
        ]
		  }
    ]
  }
}
#+end_src
And now for resolver:
#+begin_src javascript :tangle bin/0-resolver-config.json
{
	"schema": "iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-1",
	"data": {
		"cacheSize": 0,
		"repositories": [{
				"name": "Priv",
				"priority": 1,
				"vendorPrefixes": ["com.snowplowanalytics"],
				"connection": {
					"http": {
						"uri": "https://raw.githubusercontent.com/peel/schemas/master"
					}
				}
			}
		]
	}
}
#+end_src
** Encode config
With ~ammonite~:
#+begin_src shell :tangle bin/encode
#!/usr/bin/env nix-shell
#!nix-shell -i amm -p ammonite
#+end_src
run script to encode configurations.
#+begin_src scala :tangle bin/encode
import java.util.Base64
import java.nio.charset.StandardCharsets
import ammonite.ops._
import $ivy.`io.circe::circe-parser:0.13.0`, io.circe._, io.circe.syntax._, io.circe.parser._

val load = (path: String) => read! os.Path(path, base = os.pwd)
val encode = (str: String) => Base64.getEncoder.encodeToString(str.getBytes(StandardCharsets.UTF_8))

@main def run(config: String): Unit =
  parse(load(config)) match {
    case Right(data) => println(encode(data.asJson.noSpaces))
    case Left(err) => println(s"Invalid JSON input in: ${err.getMessage}")
  }
#+end_src
Setup environment:
#+begin_src shell :tangle bin/run
export JOB_CONFIG=$(./encode ./0-job-config.json)
echo   "Set up JOB_CONFIG=${JOB_CONFIG}"
export RESOLVER_CONFIG=$(./encode ./0-resolver-config.json)
echo   "Set up RESOLVER_CONFIG=${RESOLVER_CONFIG}"
#+end_src
Now use script to launch a recovery job.
** Beam
#+begin_src shell :tangle bin/beam
source run
docker run \
  -e GOOGLE_APPLICATION_CREDENTIALS=/snowplow/config/credentials.json \
  -v $HOME/wrk/sp/terraform-modules/gcp-sandbox.json:/snowplow/config/credentials.json \
  snowplow-event-recovery-beam:0.1.1 \
  --runner=DataFlowRunner \
  --job-name=snowplow-event-recovery-peel \
  --project=engineering-sandbox \
  --zone=europe-west1-b \
  --gcpTempLocation=gs://peel-test-bucket/temp \
  --inputDirectory=gs://peel-test-bucket/source/partitioned/enrichment_failures/** \
  --outputTopic=projects/engineering-sandbox/topics/recovery-peel-test \
  --unrecoveredOutput=gs://peel-test-bucket/unrecovered \
  --unrecoverableOutput=gs://peel-test-bucket/unrecoverable \
  --config=${JOB_CONFIG} \
  --resolver=${RESOLVER_CONFIG}
#+end_src
** Spark
*** Setup EMR config
#+begin_src javascript :tangle bin/0-emr-config.json
{
  "schema": "iglu:com.snowplowanalytics.dataflowrunner/ClusterConfig/avro/1-1-0",
  "data": {
    "name": "emr-recovery-cluster",
    "logUri": "s3://logs/",
    "region": "eu-central-1",
    "credentials": {
      "accessKeyId": "{{secret "aws-access-key-id"}}",
      "secretAccessKey": "{{secret "aws-secret-access-key"}}"
    },
    "roles": {
      "jobflow": "EMR_EC2_DefaultRole",
      "service": "EMR_DefaultRole"
    },
    "ec2": {
      "amiVersion": "5.17.0",
      "keyName": "some-key-name",
      "location": {
        "vpc": {
          "subnetId": "subnet-sample"
        }
      },
      "instances": {
        "master": {
          "type": "m1.medium",
          "ebsConfiguration": {
            "ebsOptimized": true,
            "ebsBlockDeviceConfigs": [
              {
                "volumesPerInstance": 12,
                "volumeSpecification": {
                  "iops": 8,
                  "sizeInGB": 10,
                  "volumeType": "gp2"
                }
              }
            ]
          }
        },
        "core": {
          "type": "m1.medium",
          "count": 1
        },
        "task": {
          "type": "m1.medium",
          "count": 0,
          "bid": "0.015"
        }
      }
    },
    "tags": [
      {
        "key": "client",
        "value": "com.snplow.eng"
      },
      {
        "key": "job",
        "value": "recovery"
      }
    ],
    "applications": [ "Hadoop", "Spark" ]
  }
}
#+end_src
*** Setup job playbook for EMR
#+begin_src javascript :tangle bin/0-emr-playbook.json
{
  "schema": "iglu:com.snowplowanalytics.dataflowrunner/PlaybookConfig/avro/1-0-1",
  "data": {
    "region": "eu-west-1",
    "credentials": {
      "accessKeyId": "{{secret "aws-access-key-id"}}",
      "secretAccessKey": "{{secret "aws-secret-access-key"}}"
    },
    "steps": [
      {
        "type": "CUSTOM_JAR",
        "name": "Staging of bad rows",
        "actionOnFailure": "CANCEL_AND_WAIT",
        "jar": "/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar",
        "arguments": [
          "--src",
          "s3n://${BUCKET_ID}/recovery/enriched/bad/run=2019-01-12-15-04-23/",
          "--dest",
          "s3n://${BUCKET_ID}/stage_01_19/"
        ]
      },
      {
        "type": "CUSTOM_JAR",
        "name": "Move to HDFS",
        "actionOnFailure": "CANCEL_AND_WAIT",
        "jar": "/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar",
        "arguments": [
          "--src",
          "s3n://${BUCKET_ID}/stage_01_19/",
          "--dest",
          "hdfs:///local/to-recover/",
          "--outputCodec",
          "none"
        ]
      },
      {
        "type": "CUSTOM_JAR",
        "name": "snowplow-event-recovery",
        "actionOnFailure": "CANCEL_AND_WAIT",
        "jar": "command-runner.jar",
        "arguments": [
          "spark-submit",
          "--class",
          "com.snowplowanalytics.snowplow.event.recovery.Main",
          "--master",
          "yarn",
          "--deploy-mode",
          "cluster",
          "s3://snowplow-hosted-assets/3-enrich/snowplow-event-recovery/snowplow-event-recovery-spark-0.2.0.jar",
          "--input",
          "hdfs:///local/to-recover/",
          "--output",
          "hdfs:///local/recovered/",
          "--config",
	  "ewogICJzY2hlbWEiOiAiaWdsdTpjb20uc25vd3Bsb3dhbmFseXRpY3Muc25vd3Bsb3cvcmVjb3Zlcmllcy9qc29uc2NoZW1hLzEtMC0wIiwKICAiZGF0YSI6IFsKICAgIHsKICAgICAgIm5hbWUiOiAiUmVwbGFjZUluQmFzZTY0RmllbGRJbkJvZHkiLAogICAgICAiZXJyb3IiOiAiaW5zdGFuY2UgdHlwZSAoc3RyaW5nKSBkb2VzIG5vdCBtYXRjaCBhbnkgYWxsb3dlZCBwcmltaXRpdmUgdHlwZSAoYWxsb3dlZDogW1wiaW50ZWdlclwiXSlcbiAgICBsZXZlbDogXCJlcnJvclwiXG4gICAgc2NoZW1hOiB7XCJsb2FkaW5nVVJJXCI6XCIjXCIsXCJwb2ludGVyXCI6XCIvcHJvcGVydGllcy9sb3RJZFwifVxuICAgIGluc3RhbmNlOiB7XCJwb2ludGVyXCI6XCIvbG90SWRcIiIsCiAgICAgICJiYXNlNjRGaWVsZCI6ICJ1ZV9weCIsCiAgICAgICJ0b1JlcGxhY2UiOiAiXCJsb3RJZFwiOlwiKFxcZCspXCIiLAogICAgICAicmVwbGFjZW1lbnQiOiAiXCJsb3RJZFwiOiQxIgogICAgfQogIF0KfQ=="
        ]
      },
      {
        "type": "CUSTOM_JAR",
        "name": "Back to S3",
        "actionOnFailure": "CANCEL_AND_WAIT",
        "jar": "/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar",
        "arguments": [
          "--src",
          "hdfs:///local/recovered/",
          "--dest",
          "s3n://${BUCKET_ID}/raw/"
        ]
      }
    ],
    "tags": [
      {
        "key": "client",
        "value": "com.snowplowanalytics"
      },
      {
        "key": "job",
        "value": "recovery"
      }
    ]
  }
}
#+end_src
*** Run EMR job
#+begin_src shell :tangle bin/spark
source run
wget http://dl.bintray.com/snowplow/snowplow-generic/dataflow_runner_0.4.0_linux_amd64.zip
dataflow-runner run-transient --emr-config ./0-emr-config.json --emr-playbook ./0-emr-playbook.json
#+end_src
*** Alternatively, start up EMR cluster
#+begin_src shell :spark-cluster
source run
wget http://dl.bintray.com/snowplow/snowplow-generic/dataflow_runner_0.4.0_linux_amd64.zip
export EMR_ID=$(dataflow-runner up --emr-config ./0-emr-config.json | rg -o '(j-.+)' -r '$1'
dataflow-runner run --emr-playbook ./0-emr-playbook.json --emr-cluster $EMR_ID
#+end_src
Stop EMR cluster
#+begin_src shell :tangle bin/spark-stop
dataflow-runner down --emr-config ./0-emr-config.json --emr-cluster $EMR_ID
#+end_src
** Flink
Assembly project
#+begin_src shell :tangle bin/flink
sbt flink/assembly
#+end_src
And run with ~docker-compose.yml~ or submit to a running Flink cluster:
#+begin_src shell :tangle bin/flink
source run
flink run -d -p 2 /opt/job.jar --input bad-rows/ --output recovered --config $JOB_CONFIG --resolver $RESOLVER_CONFIG
#+end_src
** Checking received messages
Messages delivered to Kinesis output are base64 & thrift encoded, in order to see full content, run them through console commands:
#+begin_src shell
sbt console
#+end_src
And then decode them
#+begin_src scala
eew
val body = ???
util.thrift.deserialize(body)
#+end_src
* Use REPL
#+begin_src scala
import cats.data._, cats.implicits._, cats.syntax.either
import com.snowplowanalytics.snowplow.badrows._
import com.snowplowanalytics.snowplow.event.recovery._, recoverable._, recoverable.Recoverable.ops._

val processor = Processor("aa", "bbb")
val failure = Failure.AdapterFailures(timestamp=java.time.Instant.now(), vendor="aaa", version="nono", messages=List(FailureDetails.AdapterFailure.NotJson("none", None, "error")).toNel.get)
val payload = Payload.CollectorPayload("vendor", "oo", List.empty, None, None, "oo", "ii", None, None, None, None, None, List.empty, None)
val conf: config.Config = Map(config.AdapterFailures -> List(config.Replacement("old-vendor", "old-vendor", "replace-with-me-vendor")))
BadRow.AdapterFailures(processor, failure, payload).recover(conf)
#+end_src
* Extending recovery
There are several extension points for recovery: [[Steps]], [[Conditions]] or newly added [[https://github.com/snowplow-incubator/snowplow-badrows][BadRow]] types. 
** Adding Steps
By definition [[Steps]] allow performing moditications on existing bad row data points' payloads. 
We defined a configuration DSL for Steps that is turned into actions with =Inspectable= definitions. 
Inspectables are data structures on which Steps can be performed. 
Therefore in order to add a new =Step=:
- a =config.Step= DSL structure for it has to be defined
- an implementation of action defined as per DSL has to be defined for =Inspectable=. 
The latter can be described in form of =transform= function that builds upon a recursive generic JSON-transforming structure.
** Adding Conditions
Conditions are a lot like [[Steps]] but as they are triggered earlier before =Inspectable=s are created. 
They do operate upon =JSON= data and need to implement behaviour for basic JSON types.
Therefore in order to add a new =Condition=:
- a =config.Condition= DSL structure for it has to be defined
- functions for performing =Condition= application for basic/composite JSON types have to be defined
** Adding Bad Row types
Once new recoverable Bad Row types are added they need to be turned into =Recoverable=s by supplying appropriate =Recoverable= instances.
If a new =Payload= format is added it has to be turned into =Inspectable= as well.
